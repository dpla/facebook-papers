Our global user base is never going to agree with all of our policies and enforcement
decisions. Instead, we should empower users to improve their own experience by giving them
more control over what they see (and don't see). Users agree we should take a stance and
remove what is universally thought of as bad or illegal (e.g., CEI, terrorism, violence,
harassment) but leave it up to them to decide the rest. This would take some of the strain off
of creating a content regulation system that is agreed upon by everyone.
Users feel like FB doesn't listen to its community because they have no way to give us
feedback and express their point of view. For example, appeals are generally thought of as a
way to express disagreement with a decision by an institution. However, users don't view our
appeal system as legitimate because they perceive it as just clicking a button that goes
nowhere.
a
Recommendations:
• Give users more control over what they see (and don't see). For example, users wanted
one newsfeed tab for "lighter" organic content from friends and family and another tab
for shared news articles and non-organic content. One user didn't want to see any
borderline graphic violence close to when she was going to bed. Other internal research
also provides evidence for user controls such as content covers. Other research indicates
users want the ability to dial up misinfo control (e.g., choose to have misinfo removed
from their feed) but not dial it down (i.e., remove misinfo labels).
REDACTED FOR CONGRESS
Investigate ways we can we incorporate user feedback at scale such as Oversight Board
case advocacy and community feedback
• Understand how rank and respond might further threaten the perceived 1
Aannele cutam and investicata bowtimanancamatan Lontano
Chats
