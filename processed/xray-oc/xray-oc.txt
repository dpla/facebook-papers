XRayOC 2019a clip-based model
FRIDAY, JUNE 21, 2019 · READING TIME: 17 MINUTES
This is a joint note with
Content Integrity.
from IntegrityAI team in
Redacted for Congress
TL;DR: On June 14th, we have deployed a first version of our video clip-based model,
XRayOC 2019a, for all FB traffic at upload time across all FB videos. This model features
several concepts, among them, a new first person shooter concept aimed to provide signals
that would help detecting new incidents like the NZ shooting. These concepts are available
via Video Understanding Repository.
What's new in this model
We introduced 3 new concepts into video space: vclip_car_crashing ,
vclip_cock_fighting, and vclip_first_person_shooting. Our new model is a head inside
XRayCore trunk (https://fb.workplace.com/notes)
announcing-xrayvideo-
2018a-model/356661298419058/), finetuning from comp_13_sum_4 layer.

Why clip-based?
Redacted for Congress
Historical problems: Towards the end of H1-2018, who worked on violence video
classifier, noticed that we started to observer a lot of carcrashing and cockfighting in
prevalence data. Our team worked with to collect a small set of data and train two
logistic regression concepts in our XRayOC frame-based model. However, the problem
didn't really get solved, as the model either learn to notice the appearance of two
roosters, or just recognize cars. Earlier this year, we did an exercise to label prevalence
data to see how many of them fall into these two categories, and we noticed that this is
still a problem that we need to solve. We think that we can leverage the 3D-
convolutional neural network to capture the action of cockfighting and carcrashing.
a
• Chirstchurch incident: We worked with multiple partners (Graphic Violence, AI Video
Understanding, etc), and we realized that we have been missing first-person-shooting
(FPS) detection. We think there's a strong need of developing such concept to support
our Real Time Integrity team with their operational system, so we also decided to train
this concept.

Data collection
. For carcrashing and cockfighting, we worked with PDO team to get contents enqueued
and labeled using biased sampling from our old frame-based concepts, starting from
mid-February
• For first-person shooting, data collection was more challenging. Starting from late May
7th, we get labels for contents for FPS. We leverage multiple existing systems on
Facebook systems:
Redacted for Congress
. We collected a number of videos by searching for Facebook Pages with the SERP
console and then scraping the videos from these pages. We searched for relevant
topics such as paintball, airsoft, firearms competition, police bodycam videos, etc.
Using the SERP console to find videos directly did not work very well because only
a small number of results were returned per search. Searching for pages in this
manner was more effective because a page may have tens to hundreds of videos.
o Another resource we used to find videos is IRIS (Integrity Review using Indexing
Service), which leverages the Unicorn search indices. Using IRIS/Unicorn, we can
search directly for videos using keywords for Facebook content and hashtags for
Instagram content. For Facebook videos, we can search specifically for first-
person/GoPro/helmet cam videos. An example of a query we used is
https://fburl.com/unicorn/2tcnwz7h. Each search can return up to 5000 videos,
and this is limited by Unicorn.
14
13

• Note that we did not search for the NZ video explicitly. We'll double check with our
list of known NZ videos to see if we actually did have NZ videos into our training
data or not, but to the best of our knowledge, we didn't use it for training on
purpose.
. We added around 1K video clips (user-created) for known FPS games such as Call
of Duty, Overwatch, CS-GO to be hard-negative samples.
. We filter our training data to be less than 2-minutes since we use video-level label
and apply to all clips used for training.
Redacted for Congress
Model training
Pre-training: We understand that video-deduplication is a challenging problem.
a
Therefore, we started with the most naive approach: using md5. In the later part, I'll
explain why it doesn't work well, and what are a better solution.
a
• We leverage Deep Vision framework to train our trunk on production 3D-Resnext50.
Since this is a multi label training with some videos missing labels, we use sparse
sigmoid cross-entropy loss. The production model was trained in f119151736. We
finetune the last couple of layers in this network, starting from comp_13_sum_4 blob.

The video sampling strategy is similar as in production: Every 2-seconds, we sample
every other frames, and choose the first 4-frames as the input of the network. Then we
apply video-level label for each iteration.
• The average precisions for carcrashing, cockfighting, and first-person-shooting are:
0.45965177,0.6286166,0.6067463, respectively.
Evaluation
Redacted for Congress
At Content Integrity, once we have a visual concept deployed into production, our problem
teams will consume these signals in the following manner:
• As we mentioned earlier, for every 2 seconds, we classify using 4 frames. Every 10
4
seconds, we aggregate 5 classification scores using max, median, and mean method. We
call this clip-level score.
• For VOD (video-on-demand), we aggregate all clip-level scores again using the three
main aggregation methods, and we call this video-level score. Most clients will consume
this video-level scores into their meta-classifiers, which is a classifier who'll determine if
a content is violating or not for each problems.

• For live-video, we'll write clip-level (10-second clip) scores directly into storage (VU
Repository) and trigger clients every time we write successfully as this is currently the
setup in Video Understanding Engine.
In the production system, we know:
Our production model is a quantized version of the original model that we train.
а
• Our downstream clients will use video-level scores directly for most use cases.
Therefore, we'll analyze the video-level signals in two datasets:
Redacted for Congress
.
Original evaluation dataset.
• A more fresh dataset. This is really just a new set of videos that we continuously collect
for evaluation purposes with the hope that the new data coming in will be less likely to
be similar to our training dataset. We will only do this evaluation for FPS since this has
higher priority for evaluating with more data for understanding the model quality
better.
• Note: For carcrashing and cockfighting concepts, we'll compare with our frame-based
concepts. Our goal is to deprecate our frame-based concepts to enable deploying frame-
based model faster in future, on par with our XrayOC image model.

For simplicity, we'll show the result of using max aggregation at video level, and median
aggregation at clip level. We call this median_clip_max aggregation.
Redacted for Congress
Original evaluation dataset
FPS:
The area under curve (AUC) of ROC curve is 0.8664. Below are the P/R and ROC curves.

vclip_first_person_snooting_median_clip_max P/R
1.0
0.8
0.6
Redacted for Congress
recision
0.4
0.2
P/R curve
0.0
0.0
0.2
0.4
0.6
0.8
1.0
Recal
PR curve

vclip_first_person_snooting_median_clip_max ROC
1.0
0.8
0.6
Redacted for Congress
וחבenaורובופוב
0.4
0.2
ROC curve
0.0
0.0
0.2
0.8
1.0
0.4
0.6
False nositive rate
ROC curve

Carcrashing
А
B
С
1
Aggregation
ROC AUC (frame-
based)
0.30352
ROC AUC
(clip-based)
0.98518
2
Redacted for Congress
3
0.25834
0.98514
max_clip_max
max_clip_median
median_clip_max
median_clip_median
4
0.25888
0.98429
un
5
0.23904
0.98303
In the following charts, original (in blue color) curve represents frame-based concept, and
backfilled (in orange) curve represents the clip-based concept.

car_crasning median_clip_max P/R
1.0
0.8
0.6
Precision
Redacted for Congress
0.4
0.2
P/R curve original
P/R curve backfilled
0.0
0.0
0.2
0.4
0.6
0.8
1.0
Recall
Carcrashing: P/R curve comparison of frame-based and clip-based models

car_crasning median_clip_max ROC
1.0
0.8
0.6
Redacted for Congress
2101 Banicnd anir
0.4
0.2
ROC curve original
ROC curve backfilled
0.0
0.0
0.2
0.8
1.0
0.4
0.6
False nositive rate
Carcrashing: ROC curve comparison of frame-based and clip-based models

Cockfighting
А
B
C
1
Aggregation
ROC AUC
(frame
based)
0.84576
ROC AUC
(clip-based)
Redacted for Congress
2.
0.959
3
0.81324
0.96106
max_clip_max
max_clip_median
median_clip_max
median_clip_median
4
0.81404
0.96196
5
0.74833
0.9608

COCKTignting_median_clip_max P/R
1.0
0.8
0.6
Redacted for Congress
Precision
0.4
0.2
P/R curve original
P/R curve backfilled
0.0
0.0
0.2
0.4
0.6
0.8
1.0
Recall
Cockfighting: P/R curve comparison of frame-based and clip-based models

COCKTignting_median_clip_max ROC
10 -
0.8 -
0.6
Redacted for Congress
DI Daniend ani
0.4
0.2
ROC curve original
ROC curve backfilled
0.0
0.0
0.2
0.8
1.0
0.4
0.6
False nositive rate
Cockfighting: ROC curve comparison of frame-based and clip-based models

Fresh evaluation dataset
We used one week of labeled data for this evaluation.
Redacted for Congress
For FPS, we got around 2k contents with labels:
ROC_AUC = 0.91495

vclip_first_person_snooting_median_clip_max P/R
1.0
0.8
0.6 -
Precision
Redacted for Congress
四
0.4
0.2
P/R curve
0.0
0.0
0.2
0.4
0.6
0.8
1.0
Recall
PR curve on new eval dataset

vclip_first_person_snooting_median_clip_max ROC
1.0
0.8
0.6
Redacted for Congress
201 Danicnd an
0.4
0.2
ROC curve
0.0
0.0
0.2
0.8
1.0
0.4
0.6
False nositive rate
ROC curve on new eval dataset

More analysis on FPs and FNs
Redacted for Congress
We took a lot at the evaluation dataset to understand why the model was making mistakes on
all three concepts. Here are some examples with our explanation for each concepts (mainly
focus on false positive - FP, and false negative - FN). We will try to categorize them into
common cases as we do observe some patterns of FPs and FNs in both original and fresh
evaluation dataset. We consider a content to be false positive when our signal score is high
but get labeled as negative, and a content to be false negative if our signal score is low but get
labeled as positive.
We eyeballed a lot of FPs and FNs, and we think that our model sometimes make a better
decision than our rep labeling (see the list of examples below), so in fact the model is doing
quite decent.
WARNING: The contents below might contain graphic violence. Viewer discretion is advised.
FPS
• FP:

O
452499634901094, 1421746564576717: This is somewhat considered FPS except
the person wearing bodycam didn't fire their
weapon.
• 195572491124117, 1763798177016199, 642573659435039, 1479224525496259,
1074161659301823: These are indeed FPS - all in shooting range, so there's a
problem with quality labeling.
• 757749764322572: There are a lot of 3rd person shooting perspective vs 1st person
shooting perspective.
Redacted for Congress
• 10154193493431852: We never really see the gun from the police officer, so we
didn't learn this right. There's gunshot present. The AED model in Video
Understanding Engine only capture the gunshot signal score at 5.18 (see
https://fburl.com/langtech/91n6dxm6).
O
566451300181755, 2068637626479813: This should be labeled as positive. Again,
gunshot_gunfire signal from AED is not high: 0.0197, 0.03 and 0.498 respectively.
• 935584746601531, 2068637626479813, 1025589710934367,
10154080302733664: This is similar to the previous case, but the gunshot signal
was a little bit higher: 0.4381, 0.498, 0.6048, 0.6435.
O
o 1037255302989683, 760563904140226: FPS but the gun was never fired.

• 10154489582223732: We never really see the gun fired, but we can infer that the
police officer did fire his weapon.
.
150344069107670: We should consider this as positive. If this is not real gunshot,
our acoustic detection system should be able to differentiate this rather than
leaving the visual model to learn this, especially with our production frame-
sampling mechanism.
© 224169031590957: first-person carwash ...
Redacted for Congress
• FN:
• 1043171922541322, 1775512619345470: These are clearly video games ...
© 2005960259436760, 110233993372746, 1066813513335965, 10156761234981424,
2058939074376355: These should be labeled as negative since they are third-
person perspective.
• 1003009213195604: This was edited, and even watching the footage, we didn't
really see gunshot fired, so this is a controversial sample.
10205343622226843: This is another hard sample. We don't really see the gun for
the majority of the video, but there was gunshot. We should probably considered
this sample as negative as we can capture gunshot through acoustic event detection.

• 2042777779343499, 1121762651168458, 260797567670408: Not sure why these
are labeled as positive sample when there were no gunshot fired.
O
• 1658613417748417: This video is probably indeed FPS. However, we pretty much
don't see the gun quite well in the video.
Redacted for Congress
• 10154333865096130: This is also hard. We can't really tell if the police officer fired
their weapon. From the text, it looks like the homeowner fired at those officers. We
don't really see the gun of this officer either, so we think this should be a negative
sample.
2087468144830123: This is indeed positive sample, though we don't really see the
FPS present much, so it's possible that our sampled frames didn't catch them well.
O
• 979876282153391, 531264043931536: This is another interesting example. It is
indeed FPS, but we should distinguish this from guns on land vs guns in the water.
We should not ignore this for different reasons.

542638122561762: This was the incident back in 2015 of the two reporters in
Roanoke, Virginia. This is an incident that we would capture the content. In this
particular example, this is hard to catch in production because the shooting
happens really fast, and there is a good chance that our frame-sampling didn't catch
those frames. A second hypothesis is because of the windows splitting, with our
production frame-processing (resize and crop to get the frame down to 112X112),
there is a good possibility that we would miss the gun. I'll do a follow-up check on
this particular video to see why we scored very low on this video.
a
Redacted for Congress
• 1753596228214660: This is a close-up FPS, which we should have capture. I'll do
more debugging on this sample as well.
• 1992060911048222: This should not be labeled as positive as this looks like an ad
from ammunition store. There was no gun-shot either.
Scoring live videos:
o Thanks to
we looked at contents with high FPS score in live video.
• There are a lot of false positive samples where the footage was coming from
dashcam in a car. We are aware of this problem as we do have a decent amount of
law enforcement officer in our training data.
O
Example: 2047107862251776, 670408496771722, 2288931204702120,
140110387175164.

Carcrashing
• FP:
Redacted for Congress
• 899859696863442, 496223207579240, 189805198666015, 791272874543202,
546266105779784: We didn't see the car-crashing incident itself, but we do see
dismemberment and visible innards. We do capture gory and mAD contents in our
XRayOC frame-based model, so for downstream client, it should be fine. However,
we'll revisit these examples to evaluate if we shall change our definition of
carcrashing more. Based on our labeling taxonomy, it seemed to be positive sample.
o 10155563370198562: This one is probably a hard example. We do see multiple
people are injured yet some of them seemed to be alive. This is still considered as
positive from our labeling taxonomy.

• FN:
407105813423531, 448300042593952: These are clearly cockfighting videos, and
even that it should be negative for cockfighting, but got labeled as positive
carcrashing. To make sure that we didn't make any mistakes here, we looked at
their job ids: 634977426999440 and 634977426999440 respectively. We'll follow-
up with PDO team on this.
Redacted for Congress
• 2308512196085975, 2035230543266139, 1019293531593415: These are what we
really miss. There was not a lot sign of bloods, or visible innards. We'll investigate
more into why our model didn't catch this case.
166929754319771: This video is a shooting scene video, and not carcrashing.
• 264730337748628: This video is indeed carcrashing. When the incident happened,
there was an explosion, and the visual signal to check if any person were harmed
was incredibly hard to spot.

Cockfighting
• FP:
256397441931706, 1510562725747288, 148115262862762: These are indeed
cockfighting, but we only label positive only if one of the animals are injured or
appear to be death. This is hard to catch since a lot of the positive samples are some
what in a similar setting.
a
• FN:
Redacted for Congress
o
588310181672556: This is very fast, and there's a good chance that we didn't catch
the segment when the other rooster appeared to be death.
• 819795338388874, 134393804307984: This should not be considered as positive
label from our taxonomy as none of the roosters appeared to be heavily injured.
Original NZ video
Here's the prediction of the original NZ shooting video. We perform our classification for
every 2-second. Here's the result of all classification if it was in production as of today using
median_clip_max aggregation. In the below chart, the x-axis represent the clip index, and y-
axis represents the classification score.

Redacted for Congress
100
200
300
400
500
600
700
800
900
1000
1100
1200
1300
1400
-score
Classification result of NZ video.

Note that we didn't search for NZ video directly in our training data. However, we will double
check if all copies of NZ videos were in our training dataset or not.
Existing Problems & Future Work
Here are some main problems that we surface, and what we should focus on:
Redacted for Congress
Training data deduplication
• For XRayOC image, we have a continuous curation pipeline, where we run
clustering algorithm and perform label aggregation, which gives us more confident
in our train/eval split. We use PhotoDNA for such task. For video, this task is
harder. Our first solution with md5 does not work well, as we see a lot of identical
videos with different edits (text-overlaid, cropped, distortion, etc).
• We will start looking deeper into video deduplicátion. Some of the first solutions
that we think will be better are using existing systems at Facebook: Ridge,
VIdentifier. We plan to work with Video Compression team in H2 to use this to
clean up our training data.

• Our metrics on car-crashing and cock-fighting look suspicious and too good to be
true. We did perform eyeballing result and see a lot of disagreement in terms of
labels. Therefore, we think once we are able to deduplicate properly, we'll have a
better understanding on the model performance, and it will guide us to figure out
how to improve our clip-based model.
• Label taxonomy and concept training
Redacted for Congress
• We learn that our first labeling taxonomy is not granular enough. We only check if
there's a real gun fired in a first person perspective or not. We should think about
including more questions to get better granular labels, and also enable multi-label
selections for our PDO labeling queue.
• Label quality
• As we mentioned above, there were a lot of examples mis-labeled in these main
categories:
• Games video labeled as positive.
• 3rd person perspective is marked as ist person perspective.
. No gun visible but labeled as FPS.
. We've reached out to PDO and we'll work with their team to ensure the quality.
There were quite a lot of problems when we eyeballed FPs and FNs, so it was quite
hard for us to debug the real FPs and FNs in the first iteration.

. We've reached out to PDO and we'll work with their team to ensure the quality.
There were quite a lot of problems when we eyeballed FPs and FNs, so it was quite
hard for us to debug the real FPs and FNs in the first iteration.
• Better audio understanding
Redacted for Congress
• It is clear that with visual signal only, we will miss a decent amount of contents. At
Realtime Integrity team, once they work on a new classifier, learning both from
visual and audio would benefit their system drastically. Today, our acoustic event
detection signals in Video Understanding Engine (VUE) are still missing a lot of
events, or the quality of detecting gunshots are still not clear to us. We would love
to work with partner teams owning this system and improve audio recognition.
• Define more operational metrics
O
Today, we evaluate our concept performance through how we serve our signals
through VUE. These are served in both clip-level and video-level scores. Therefore,
we evaluate our concepts based on this assumption, mainly on video-level scores.
We plan to spend more time to understand which metrics would benefit the most to
both our team and downstream clients (Graphic Violence, Realtime Integrity).

Acquire clip-level label
• Another main problem with using video-level label for each videos is that we are
missing out a lot of potential good training data. We plan to build a new system that
can smart select clips from videos (loudest clips) and acquire the labels rather than
using its entire video. This way, our training data will be more granular, and the
label is very likely to be more accurate.
Surface signals to all IG traffic
Redacted for Congress
• As part of vio deprecation, AI Video Understanding team has already been working
with IG Well-being team to transition from vio to XRayVideo 2018a. Once this
transition is complete, these concepts will be available to all IG videos (both VOD
and live)
Deprecate carcrashing and cockfighting frame-based concept
. We have been working to deprecate these two concepts to support the deployment
of new XRayOC frame-based model. We have a new candidate model ready, and I'll
share the result in my next note. We have worked with Video Infra team to enable
online A/B testing for all plugins across VUE platform, and we'll be starting to use
this to support all downstream clients in Content Integrity to support a new
deployment of XRayOC video frame-based model.

Incorporate the new signal into downstream clients
• We will work with Graphic Violence and Realtime Integrity team to incorporate
these signals into their meta-classifiers. cc
and
Work with our outreach team to collaborate with law enforcement
Redacted for Congress
. We would love to acquire data from law enforcement, mainly their bodycam
footage. We've just received the first copies of these from USG. Our team will
evaluate and figure out how to best use these footage for our system.
We hope with these work can be done, our clip-based model will increase its performance.
Our team's goal for clip-based model is to fill the gap from what our frame-based model
cannot learn, so we'll work on identifying problems that required action recognition together
with vertical teams in Content Integrity.

Acknowledgement
This is a very challenging project, and our work is just 1% complete. we would like to thank
all partners who have supported our team on this project.
• PDO:
and the PDO labeling reps at Sao Paulo, Gdansk, and Hyderabad sites.
.
IntegrityAI (previously IntegrityCV):
Redacted for Congress
• Content Integrity
• Games:
• AI Video Understanding - AML:
• Video Infra:
• Policy:
4
Please let me know if I'm missing your name in this, I apologize in advance and will correct
that.

- probably for you
you o
1
Thanks,
I think you're right.
1
Redacted for Congress
O Haha my bad. Let me fix this
Like · Reply · 1y
#sharebot Games ML Working Group
LIKE Reply
It doesn't work
Like · Reply · 1y
Great work Very exited to support OC clip-based model inside the XRayCore trunk. Looking forwards to
the comparison with frame-based carcrash/cockfighting concepts!
Like · Reply · 1y
O We did compare this with our offline data as I mentioned above. It's probably a lot better though in a
not-super-clean dataset. I assume what you meant is how this would reflect in our downstream clients in graphic
violence? If so, yes. We'll work with them on refreshing their classifier in a very short future.
Like · Reply · 1y
Yes that what I meant, it's would be great to see the impact to downstream clients.
Like · Reply · 1y

Like · Reply - ly
Yes that what I meant, it's would be great to see the impact to downstream clients.
1
Like · Reply · ly
Pointer to the car crashing data you wanted!
Like · Reply · 1y
2
Great work everyone! Exciting to see the improvement even with regard to the original NZ video. Looking
forward to the future work too and get this productionalized for Live.
Like · Reply · 1y
C
LIKE Tepuy
Great work! Thanks for sharing your learnings here. Looking forward to seeing where this goes!
Like · Reply - Ty
1
Do we know what percentage of FPS videos are actually violating? My concern is that we build a model that
detects FPS videos very well - but if a very small percentage of FPS videos are actually violating, it doesn't make sense to
enqueue jobs based on this score.
Like · Reply - 1

Like · Reply · 1y
Redacted for Congress
We don't know the exact volume at this point, and we think this is going to be incredibly small
Our first stage is to provide visual understanding (and probably improve audio recognition) for such detection system.
At this stage, our focus is not to determine if a video is actually violating or not. Our next stage is going to buy a
classifier which will gather all knowledges that we know about such content and will determine if it is violating, and this
will be the front-line detection for such violating videos, and this effort is led by
team. Note that for all of these
effort, we are tackling live videos. Real Time Integrity Working Group is probably the best place to learn more about our
work in terms of operationalizing live video problems.
For VOD, we are in the discussion with Graphic Violence team to incorporate such signals into their video violence
classifier (cc
We expect this to be done in Q3.
3
Like · Reply · 1y. Formatted
Cool, thanks for additional info! Sounds like I'm jumping the gun & the next step will be to incorporate
these learnings into a violating classifier.
Like · Reply · 1y
1
Really interesting work, and great to see the progress.!
Bad
LIKE Reply
n GO
Great write-up! Any chance you could re-attach the Unicorn query link? Looks like it's stale. I'm also
working on collecting egocentric videos (in a different context).
Like · Reply - Ty
Followed up offline :)
Like · Reply · ly

Like · Reply · 1y
Operationally if the NZ video were uploaded today - is the new model going to flag it automatically for
remove or removal? E.g. does it hit the min thresholds?
Like · Reply · 1y
Redacted for Congress
This model aims to learn the visual understanding of first-person shooter perspective which all of our computer vision
systems have not been able to capture previously; therefore we are not aiming at enqueuing contents directly using this
concept. There are multiple reasons:
Today, mining for training data is incredibly hard for this concept, and labeling is also very tricky. We have
completed our first iteration about a month ago, and we learned there were a lot of videos like bodycam infront of
cars, so we would have a lot of FPs. We are working on multiple angles to solve this problem from cleaning up
data, acquiring more data, and train our clip-based model more effectively.
• From Content Integrity, we aim to build a classifier (led by Real-time Integrity team - cc|
to build the
enforcement classifier. To my best understanding, we are working on live video problem together with multiple
partners from both enhancing infrastructure to content understanding for incredibly rare types of contents such
as shooting incidents, SSI, etc.
Today, in the absence of the classifier as the team is working on it, we do enqueue contents with very high threshold
(0.98), and the NZ videos might not make it to the queue due to video quality, and CO capacity.
Like · Reply · ly. Formatted
3 Вас
do we have an updated answer to this question? E.g. today would the NZ video be
nG
flaged as it was live?
Like · Reply · ly

1
Redacted for Congress
Here's how I derived an answer for you:
• After I test with the original model (where max score across the entire video is 0.96 as I wrote in my note), I took
some more known copies of NZ videos and re-stream this video. A lot of known copies of NZ was altered with
different dimensions, overlaid text, therefore the max score that we get from the current production model were in
different range in (0.8 -0.96).
• The limitation of the production model is it is somewhat biased towards dashcam, where we observed a significant
amount of law enforcement videos in our training data.
• We are limited in labeling resources from Community Ops (cc|
and with such high false positive rate,
we have to leave the threshold really nigh. Today, CO can only review - 100-200 live videos reported for this report
type, so we'll have to do better job on our modeling end.
Therefore, the answer is likely no at the moment (and yes only if we get the exact original video) for the FPS concept
itself. That being said, our progress tor turn this answer to a yes:
• Today, we are really short of "proxy" positive data given the super low volume of such category. We aim to solve
this problems by:
. Work with law enforcement outreach (cc|
to collect more
data. So far we got some data from National Counter-Terrorism Center) but the volume is in-significant. We're
working on partnering with London MET police to acquire more of their office training data.
• Prepare better training data. Since we don't have the ability of localizing the "positive" part of the video, we
have to come up with a better algorithm since we can potential mark an 8-hour long video to be positive even if Ba
20-second of that matters.
Like · Reply · 1y. Edited . Formatted
nG
unrelated to the FPS concept, the new graphic violence Live video model we launched early this week
consistently scores the NZ video in the 90s, well above the report threshold of 0.66.
For this reason, the answer to your question is yes. If re-streamed today, the NZ video would be proactively enqueued
for human review.
I have confirmed the original NZ video was not in training, however, I will need to do further analysis before confidently
confirming that no copies of the video were in the training set.
Like · Reply · 1y. Edited
1
4

Like · Reply · Ty Edited
To clear the confusion, the FPS model alone described in the note is a pure visual model. model
is referring to a meta-classifier that consumes FPS signal among with other visual and acoustic features (such as
gunshot, machine gun, etc), which we think should be a model that we aim to use in production. In Content Integrity,
this is the two-stage system that we've been using which proved to be very effective in other high-prevalence problems
1
Like · Reply. Ty
And do we have the capacity to review it? E.g. would it be enqueued and reviewed and
taken down?
Like · Reply. Ty
Redacted for Congress
Is there more to read about the new GV live video model?
Like · Reply · 1y
yes. The GV live video model scores the NZ shooting high enough that it would be
enqueued for review into a queue with a 5 minute SLA. If reviewed correctly, it would be taken down following the
review.
I have not yet published a note on the new model. I plan to have this out by the end of the quarter (ie. within the next
two weeks)
Like · Reply. Ty
note on new model here
1
Like · Reply · 1y. Formatted
